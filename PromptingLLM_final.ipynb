{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXV3R03H6rJnE69TQjabiL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afeefapp/Enhanced_ASAG/blob/main/PromptingLLM_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq\n",
        "!pip install python-dotenv==1.0.0\n"
      ],
      "metadata": {
        "id": "lxONB8o5Jery",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f10dba6-13f1-4b8f-f388-aa3ace08ad2b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n",
            "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.11.0\n",
            "Collecting python-dotenv==1.0.0\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Genearation of QA pairs for a chapter.\n",
        "The 'DataProcessed' folder contain 'merged_html' file for each chapter separately. You need to upload both .env file and 'merged_html' file begore running this code. The output will be a set of json files, each contain QA pairs for each chunk and the context (chunk itself).   "
      ],
      "metadata": {
        "id": "6akCLafPJr0C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DsxaqAZOJRYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55af32d8-015a-4d11-84da-63a3b4dc687f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Chapter QA saved to chapter_qa0.json\n",
            "45000\n",
            "1\n",
            "Chapter QA saved to chapter_qa1.json\n",
            "33000\n",
            "2\n",
            "Chapter QA saved to chapter_qa2.json\n",
            "21000\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Optional, Dict, Union\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "from pydantic import BaseModel, Field\n",
        "from groq import Groq\n",
        "import os\n",
        "from bs4 import BeautifulSoup  # Import BeautifulSoup\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "client = Groq(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
        ")\n",
        "\n",
        "groq = Groq()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Data model for LLM to generate\n",
        "\n",
        "class Question(BaseModel):\n",
        "    question: str\n",
        "    answer: Union[str, Dict]\n",
        "\n",
        "class ChapterQA(BaseModel):\n",
        "    questions: List[Question]\n",
        "\n",
        "\n",
        "def generate_qa_from_chapter(script_content: str) -> ChapterQA:\n",
        "\n",
        "    \"\"\"\n",
        "    Generate maximum number questions-answer pairs for students based on the provided script_content.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an assistant who creates questions and corresponding answers for educational use based on the provided script content.\n",
        "    Question complexity should be medium level.\n",
        "\n",
        "    **Instructions:**\n",
        "    1. **Exclude questions related to the examples, figures or diagrams.** Focus on textual content.\n",
        "    2. **Generate maximum number of unambiguous, well-defined question-answer pairs for students based on the provided script_content.**\n",
        "    Ensure each question clarifies potentially broad terms, specifies the scope of the topic, and indicates the level of detail expected.\n",
        "    Avoid vague terms like ‘other’, ‘etc.' or 'equation number' and provide guidance within the question if multiple interpretations might arise.\n",
        "    For example, specify particular concepts or terms if they need further elaboration to avoid confusion.\n",
        "    3. **Ensure that answers are thoroughly address all concepts directly required by the question.**\n",
        "    4. **For answers that include an equation, provide a brief description of each term in the equation.**\n",
        "\n",
        "    **Examples of Desired Questions:**\n",
        "\n",
        "    * \"Explain the concept of angular momentum in the context of rotational motion.\"\n",
        "    * \"What are the key differences between center of mass and center of gravity?\"\n",
        "    * \"Using the equation for torque, calculate the force required to rotate an object with a given moment of inertia.\"\n",
        "\n",
        "    **Examples of Undesired Questions:**\n",
        "\n",
        "    * \"What are some things related to rotational motion?\" (Too broad)\n",
        "    * \"Discuss the importance of this equation.\" (Unclear what \"this\" refers to)\n",
        "    * \"What else can we learn from this chapter?\" (Vague and open-ended)\n",
        "\n",
        "    Script Content: {script_content}\n",
        "\n",
        "    Output the questions and answers in the following JSON format:\n",
        "    json {{ \"questions\": [ {{ \"question\": \"Question here\", \"answer\": \"Answer to the question\" }},  # ... more questions\n",
        "    ]}}\n",
        "    \"\"\"\n",
        "    chat_completion = groq.chat.completions.create(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": prompt},\n",
        "        ],\n",
        "        model=\"llama3-70b-8192\",  # You might need to adjust the model\n",
        "        temperature=0,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "    )\n",
        "    return ChapterQA.model_validate_json(chat_completion.choices[0].message.content)\n",
        "\n",
        "\n",
        "def save_qa_to_json(chapter_qa: ChapterQA, filename: str, context: str):\n",
        "    \"\"\"Saves the chapter QA data to a JSON file.\"\"\"\n",
        "    chapter_qa_json = chapter_qa.model_dump()\n",
        "    chapter_qa_json[\"context\"] = context\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump(chapter_qa_json, f, indent=4)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "with open(\"/content/merged_output.html\", \"r\") as f:  # Replace with your HTML file\n",
        "    html_content = f.read()\n",
        "\n",
        "soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "script_content = \"\"\n",
        "for script_tag in soup.select_one(\"script\"):\n",
        "    script_content += script_tag.string if script_tag.string else \"\"\n",
        "\n",
        "chunk_size = 15000\n",
        "overlap = 3000\n",
        "chunks = []\n",
        "for i in range(0, len(script_content), chunk_size - overlap):\n",
        "  chunk = script_content[i:i + chunk_size]\n",
        "  chunks.append(chunk)\n",
        "\n",
        "max_index = len(chunks) * chunk_size\n",
        "\n",
        "for i in range(len(chunks)):\n",
        "  start_index = i * (chunk_size - overlap)\n",
        "  end_index = start_index + chunk_size\n",
        "  if end_index > max_index:\n",
        "    end_index = max_index\n",
        "  chunk = script_content[start_index:end_index]\n",
        "  print(i)\n",
        "  chapter_qa = generate_qa_from_chapter(chunk)\n",
        "  save_qa_to_json(chapter_qa, 'chapter_qa'+str(i)+'.json', chunk)\n",
        "  print('Chapter QA saved to chapter_qa'+str(i)+'.json')\n",
        "  print(max_index-start_index)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zZOL7mRGO5F",
        "outputId": "e79b3c5f-37ef-4da0-d19c-41e15aa32e08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run Only When Errors Occur\n",
        "This code can be used if certain chunks cause failures during JSON file generation, allowing you to skip the problematic chunk.\n",
        "\n",
        "For instance, if the chunk named 'chapter_qa2' was saved last before the error occurred, it indicates that the 3rd chunk encountered issues. In that case, you can adjust the start index of the for loop to '4' and run the code. If similar errors occur again, you can repeat this process. If the above code runs without any errors, this code can be skipped."
      ],
      "metadata": {
        "id": "GMxaweI_LAKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5, len(chunks) + 1):\n",
        "  start_index = i * (chunk_size - overlap)\n",
        "  end_index = start_index + chunk_size\n",
        "  if end_index > max_index:\n",
        "    end_index = max_index\n",
        "  chunk = script_content[start_index:end_index]\n",
        "  #print(i)\n",
        "  chapter_qa = generate_qa_from_chapter(chunk)\n",
        "  save_qa_to_json(chapter_qa, 'chapter_qa'+str(i)+'.json', chunk)\n",
        "  print('Chapter QA saved to chapter_qa'+str(i)+'.json')\n",
        "  print(max_index-start_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "xCdx4DirBAvn",
        "outputId": "f590dc43-1d40-4cdd-808c-15b7f5dd76d1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jbe9q8bge1etp8rq5qzp1qa5` on : Limit 500000, Used 497203, Requested 4220. Please try again in 4m5.864199999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-d4c28a8dd8dc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript_content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m#print(i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mchapter_qa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_qa_from_chapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0msave_qa_to_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchapter_qa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'chapter_qa'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Chapter QA saved to chapter_qa'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-0a2819b798a3>\u001b[0m in \u001b[0;36mgenerate_qa_from_chapter\u001b[0;34m(script_content)\u001b[0m\n\u001b[1;32m     67\u001b[0m     ]}}\n\u001b[1;32m     68\u001b[0m     \"\"\"\n\u001b[0;32m---> 69\u001b[0;31m     chat_completion = groq.chat.completions.create(\n\u001b[0m\u001b[1;32m     70\u001b[0m         messages=[\n\u001b[1;32m     71\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    285\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \"\"\"\n\u001b[0;32m--> 287\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1242\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         )\n\u001b[0;32m-> 1244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 936\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    937\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama3-70b-8192` in organization `org_01jbe9q8bge1etp8rq5qzp1qa5` on : Limit 500000, Used 497203, Requested 4220. Please try again in 4m5.864199999s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': '', 'code': 'rate_limit_exceeded'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To Merge All JSON Files into a Single JSON for the Entire Chapter\n",
        "\n",
        "Adjust the following settings before running:\n",
        "\n",
        "\n",
        "\n",
        "*   Chapter_number:\"\"\n",
        "*   Chapter_name: \"\"\n",
        "*   output file_name (set according to your convinience)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DYm-2HJxNvFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def merge_json_files(file_paths, output_file):\n",
        "    merged_data = []\n",
        "    count = 0\n",
        "    for path in file_paths:\n",
        "        with open(path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            count = count + (len(data['questions']))\n",
        "            merged_data.append(data)\n",
        "    print(f\"The number of question-answer pairs in this chapter = {count}\")\n",
        "    merged_json= {\n",
        "        \"chapter_number\": \"02\",\n",
        "        \"chapter_name\": \"MATHEMATICALMODELLING\",\n",
        "        \"question_answers\": merged_data  # Store the merged data under a 'data' key\n",
        "    }\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        json.dump(merged_json, outfile, indent=4)\n",
        "\n",
        "files = [f for f in os.listdir('/content') if os.path.isfile(os.path.join('/content', f)) and f.lower().endswith(('.json'))]\n",
        "\n",
        "merge_json_files(files, '/content/ch-02.json')\n"
      ],
      "metadata": {
        "id": "8L4gQlUYJasS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b21db7fb-653b-4e92-f760-7761e5c4e0ae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of question-answer pairs in this chapter = 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To download the merged JSON"
      ],
      "metadata": {
        "id": "w1iB8YYOPISb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/ch-a2.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "xDtDHDNahd9b",
        "outputId": "05317381-c7c6-43f9-c4c7-c6c1d158fc0b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_56365cf5-d0e4-4021-bf4f-9177786e19bf\", \"ch-a2.json\", 38378)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To Clear All JSON and HTML Files in the /content Directory\n",
        "\n",
        "Run this code to clean up the /content folder before starting a new QA generation process."
      ],
      "metadata": {
        "id": "yU9UedX3PcHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the directory to clean\n",
        "directory = '/content'\n",
        "\n",
        "# Iterate through all files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    # Check if the file is a JSON or HTML file\n",
        "    if filename.endswith((\".json\", \".html\")):\n",
        "        # Construct the full file path\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        # Attempt to delete the file\n",
        "        try:\n",
        "            os.remove(file_path)\n",
        "            print(f\"Deleted: {filename}\")\n",
        "        except OSError as e:\n",
        "            print(f\"Error deleting {filename}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0uCDb8xjItY",
        "outputId": "aab72339-a962-4892-8d57-9352b18b1628"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted: chapter_qa1.json\n",
            "Deleted: chapter_qa0.json\n",
            "Deleted: merged_output.html\n",
            "Deleted: chapter_qa2.json\n",
            "Deleted: ch-a2.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mLTm_EGmhfWY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}